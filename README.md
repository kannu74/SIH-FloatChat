<!-- Main Title and Description -->
<h1 align="center">FloatChat - AI Conversational Interface for ARGO Data</h1>
<p align="center">
  This project is an AI-powered conversational system for ARGO float data that enables users to query, explore, and visualize oceanographic information using natural language.
</p>

---

<!-- Phase 1 Section -->
<h2>Phase 1: Data Foundation & Backend Setup</h2>
<p>
  This first phase focuses on building a robust data pipeline. The goal is to ingest raw ARGO NetCDF data files, process them into a structured format, and store them in a relational database (PostgreSQL) and a vector database (ChromaDB) for AI-powered retrieval.
</p>

<!-- Project Structure Section -->
<h3>Project Structure and File Explanations</h3>
<p>The file structure for this initial phase is as follows:</p>
<pre><code>
sih/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ data_processing/
â”‚   â”‚   â””â”€â”€ processor.py
â”‚   â””â”€â”€ database/
â”‚       â””â”€â”€ setup_db.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”œâ”€â”€ db/
â”œâ”€â”€ .env
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ run_ingestion.py
</code></pre>
<ul>
    <li><code><b>backend/</b></code>: This directory contains all the server-side Python logic, separated into modules.</li>
    <li><code><b>backend/data_processing/processor.py</b></code>: A script responsible for reading a single ARGO NetCDF (<code>.nc</code>) file, extracting key variables (like temperature, salinity), and converting them into a structured format (a pandas DataFrame).</li>
    <li><code><b>backend/database/setup_db.py</b></code>: This script connects to the PostgreSQL database and creates the necessary tables (<code>argo_floats</code> and <code>argo_measurements</code>) required to store the ARGO data.</li>
    <li><code><b>data/raw/</b></code>: This is where you must place the raw <code>.nc</code> ARGO data files you download. The ingestion script specifically looks in this folder.</li>
    <li><code><b>db/</b></code>: This directory is automatically generated by ChromaDB to store the local vector database files.</li>
    <li><code><b>.env</b></code>: A file to store sensitive information and environment variables, such as database credentials. This file is ignored by Git.</li>
    <li><code><b>docker-compose.yml</b></code>: Defines the services, networks, and volumes for our Docker application. In this phase, it's used to easily configure and run our PostgreSQL database.</li>
    <li><code><b>run_ingestion.py</b></code>: The main script for Phase 1. It orchestrates the entire pipeline: it finds data files in <code>data/raw/</code>, uses the <code>processor.py</code> to process them, and loads the structured data into both PostgreSQL and ChromaDB.</li>
</ul>

---

<!-- Installation Section -->
<h2>Step-by-Step Installation Guide</h2>

<h3>1. Prerequisites</h3>
<p>Ensure you have the following software installed on your system:</p>
<ul>
  <li><strong>Git:</strong> For cloning the repository.</li>
  <li><strong>Python 3.8+</strong> and Pip.</li>
  <li><strong>Docker Desktop:</strong> For running the PostgreSQL database in a container. It's the cleanest and most reliable way to manage the database.
    <ul>
      <li><a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank">Install Docker on Windows</a></li>
      <li><a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank">Install Docker on Mac</a></li>
      <li><a href="https://docs.docker.com/desktop/install/linux-install/" target="_blank">Install Docker on Linux</a></li>
    </ul>
  </li>
</ul>

<h3>2. Clone the Repository & Set Up Environment</h3>
<ol>
  <li><strong>Clone the repository:</strong>
    <pre><code>git clone &lt;your-repository-url&gt;
cd sih</code></pre>
  </li>
  <li><strong>Create and activate a virtual environment:</strong>
    <pre><code># On Windows
python -m venv venv
.\venv\Scripts\activate

# On macOS/Linux
python3 -m venv venv
source venv/bin/activate</code></pre>
  </li>
  <li><strong>Install the required Python packages:</strong>
    <pre><code>pip install -r requirements.txt</code></pre>
  </li>
</ol>

<h3>3. Configure Environment Variables</h3>
<ol>
  <li>In the root `sih/` folder, create a file named <strong>.env</strong>.</li>
  <li>Copy the following content into it. This password will be used to secure your database.</li>
</ol>
<pre><code>POSTGRES_USER=argo_user
POSTGRES_PASSWORD=your_secure_password
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=argo_db</code></pre>
<p><em><strong>Note:</strong> Replace <code>your_secure_password</code> with a password of your choice.</em></p>

<h3>4. Run the Database using Docker</h3>
<p>With Docker Desktop running, use Docker Compose to manage your database container. These commands should be run from the root <code>sih/</code> directory.</p>
<pre><code># Start the database container in the background
docker-compose up -d</code></pre>

<h4>Useful Docker Commands</h4>
<ul>
    <li><strong>Check container status:</strong>
        <pre><code>docker-compose ps</code></pre>
    </li>
    <li><strong>View real-time database logs (for debugging):</strong>
        <pre><code>docker-compose logs -f db</code></pre>
    </li>
    <li><strong>Stop and remove the container:</strong>
        <pre><code>docker-compose down</code></pre>
    </li>
</ul>


<h3>5. Download Data & Run the Ingestion Pipeline</h3>
<ol>
  <li><strong>Download sample ARGO data:</strong>
    <ul>
      <li>Go to the <a href="ftp://ftp.ifremer.fr/ifremer/argo/geo/indian_ocean/" target="_blank">Indian Ocean ARGO Data Repository</a>.</li>
      <li>Download a few <strong>.nc</strong> (NetCDF) files.</li>
      <li>Place these files inside the <strong><code>sih/data/raw/</code></strong> directory.</li>
    </ul>
  </li>
  <li><strong>Run the setup and ingestion scripts:</strong>
    <pre><code># First, create the tables in the database
python backend/database/setup_db.py

# Next, process the .nc files and load the data
python run_ingestion.py</code></pre>
  </li>
</ol>
<p>After the `run_ingestion.py` script finishes, Phase 1 is complete! Your database is now populated and ready for the AI backend.</p>

---

<!-- Prompt for Teammates Section -->
<h2>ðŸš€ Prompt for Teammates (Gemini/ChatGPT)</h2>
<p>To help other teammates get set up quickly, provide them with the following prompt to use in an AI assistant like Gemini or ChatGPT.</p>

<div style="background-color: #f0f0f0; border-left: 5px solid #007bff; padding: 15px; margin-top: 20px; font-family: monospace; white-space: pre-wrap;">
You are an expert technical assistant. Your goal is to guide me, a software developer, through setting up Phase 1 of the 'FloatChat' project on my local machine.

I have already cloned the project repository. Provide me with a clear, step-by-step set of commands and instructions to get the data pipeline running.

Use the following information to structure your response:
1.  **Prerequisites:** Remind me that I need Python 3.8+ and Docker Desktop installed. Provide the official download links for Docker.
2.  **Virtual Environment:** Give me the commands to create and activate a Python virtual environment for both Windows and macOS/Linux.
3.  **Dependencies:** Tell me the command to install the required packages using the `requirements.txt` file.
4.  **Environment Configuration:** Instruct me to create a `.env` file in the project root and provide the exact content it should have. Tell me to set a secure password.
5.  **Database Setup:** Provide the `docker-compose up -d` command to start the PostgreSQL server and explain briefly what it does.
6.  **Data Ingestion:** Instruct me to download sample ARGO `.nc` files from `ftp://ftp.ifremer.fr/ifremer/argo/geo/indian_ocean/` and place them in the `sih/data/raw/` folder.
7.  **Run the Pipeline:** Provide the final two Python commands (`python backend/database/setup_db.py` and `python run_ingestion.py`) in the correct order and explain what each one does.

Please be concise and use markdown code blocks for all terminal commands.
</div>

<h1>Phase 2: The AI Backend API ðŸ§ </h1>
<p>
  This phase focuses on building the "brain" of the FloatChat application. We will create a backend API server using Flask that accepts user questions in natural language, uses Google's Gemini AI model to convert them into SQL queries, executes those queries against our PostgreSQL database, and returns the results.
</p>

<hr>

<h2>Updated Project Structure</h2>
<p>
  New files and folders for Phase 2 are highlighted in bold. We've added a dedicated <code>api</code> folder for server endpoints, an <code>llm</code> folder for the AI logic, and a main <code>app.py</code> to run the server.
</p>
<pre><code>
sih/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/              <b>&lt;-- NEW FOLDER</b>
â”‚   â”‚   â””â”€â”€ main.py       <b>&lt;-- NEW FILE</b>
â”‚   â”œâ”€â”€ data_processing/
â”‚   â”‚   â””â”€â”€ processor.py
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â””â”€â”€ setup_db.py
â”‚   â””â”€â”€ llm/              <b>&lt;-- NEW FOLDER</b>
â”‚       â””â”€â”€ rag_handler.py  <b>&lt;-- NEW FILE</b>
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”œâ”€â”€ db/
â”œâ”€â”€ .env
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt      <b>&lt;-- UPDATED</b>
â””â”€â”€ app.py                <b>&lt;-- NEW FILE</b>
</code></pre>

<hr>

<h2>New & Updated File Explanations</h2>
<div>
  <h3><code>requirements.txt</code> (Updated)</h3>
  <p>
    This file is updated to include two new major libraries: <b>Flask</b>, a micro web framework for building the API server, and <b>google-generativeai</b>, the official Python SDK to interact with the Google Gemini AI model.
  </p>

  <h3><code>backend/llm/rag_handler.py</code></h3>
  <p>
    This is the core AI logic module. Its primary function, <code>get_sql_from_question</code>, takes a user's question, combines it with the database schema and context from ChromaDB, and sends it all to the Gemini model in a carefully crafted prompt. The model's response is the generated SQL query.
  </p>

  <h3><code>backend/api/main.py</code></h3>
  <p>
    This script sets up the <b>Flask</b> web server. It defines the main API endpoint at <code>/api/chat</code> which listens for <code>POST</code> requests. When a request is received, it extracts the user's question, calls the <code>rag_handler</code> to get a SQL query, executes that query on the PostgreSQL database, and returns the data as a JSON response.
  </p>

  <h3><code>app.py</code></h3>
  <p>
    This is the main entry point to start the entire backend application. It simply imports the Flask <code>app</code> object from <code>backend.api.main</code> and runs it, making the server accessible on your local machine.
  </p>
</div>

<hr>

<h2>How to Run and Test Phase 2 ðŸš€</h2>
<p>
  Follow these steps to get your AI backend running and test its functionality.
</p>
<ol>
  <li>
    <strong>Get Your Google AI API Key</strong>
    <p>Go to <a href="https://aistudio.google.com/" target="_blank">Google AI Studio</a>, generate a new API key, and add it to your <code>sih/.env</code> file.</p>
    <pre><code>
# ... your other variables ...
GOOGLE_API_KEY="your_google_api_key_here"
    </code></pre>
  </li>
  <li>
    <strong>Install New Dependencies</strong>
    <p>With your virtual environment activated, install Flask and the Google AI library.</p>
    <pre><code>pip install -r requirements.txt</code></pre>
  </li>
  <li>
    <strong>Run the API Server</strong>
    <p>First, ensure your PostgreSQL database is running via Docker (<code>docker-compose up -d</code>). Then, start the Flask application.</p>
    <pre><code>python app.py</code></pre>
    <p>You should see output indicating the server is running, like: <code>* Running on http://127.0.0.1:5000</code></p>
  </li>
  <li>
    <strong>Test the API Endpoint âœ…</strong>
    <p>Open a <strong>new, separate terminal</strong> (do not close the server). Use the <code>curl</code> command to send a question to your running API.</p>
    <pre><code>curl -X POST -H "Content-Type: application/json" -d "{\"question\": \"Show me the 5 most recent temperature and salinity measurements\"}" http://127.0.0.1:5000/api/chat</code></pre>
  </li>
</ol>

<hr>

<h2>Verifying Success</h2>
<p>
  A successful test will return a JSON object in your terminal. You can confirm everything is working if:
</p>
<ul>
  <li>The JSON contains your original <code>"question"</code>.</li>
  <li>It contains a valid <code>"sql_query"</code> generated by the AI.</li>
  <li>The <code>"data"</code> array is populated with results from your database.</li>
</ul>
<p>
  <strong>Troubleshooting:</strong> If the <code>"data"</code> array is empty (<code>"data": []</code>), it means your AI and API are working, but your database tables are empty. Stop the server (Ctrl+C), run <code>python run_ingestion.py</code> to load the data, and then restart the server.
</p>
<!-- Title for Phase 3 -->
<h2>Phase 3: Interactive Frontend Chat Interface</h2>
<p>
  With the backend AI and data pipeline established in the previous phases, Phase 3 focuses on building the <strong>User Interface (UI)</strong>. This phase creates a web-based, interactive chat application that allows users to communicate with the AI backend in a familiar, intuitive way. We used HTML, CSS, and JavaScript to build a functional and responsive frontend.
</p>

<h3>Key Features Implemented</h3>
<ul>
  <li><strong>Chat Interface:</strong> A simple, dark-themed chat window to send and receive messages.</li>
  <li><strong>Context Memory:</strong> The application uses the browser's <code>localStorage</code> to remember conversations, allowing users to close the tab and return to their chat history later.</li>
  <li><strong>Multiple Chat Sessions:</strong> A sidebar lists all conversations, enabling users to switch between different topics or start new chats.</li>
  <li><strong>Dynamic Data Display:</strong> The frontend can receive data from the backend and dynamically render it as a table directly in the chat window.</li>
  <li><strong>UI Enhancements:</strong>
      <ul>
          <li>A "FloatChat is thinking..." indicator with a spinner while waiting for a response.</li>
          <li>A collapsible view to inspect the exact SQL query generated by the AI.</li>
          <li>Functionality to delete unwanted chat sessions.</li>
          <li>A placeholder for the future 3D avatar.</li>
      </ul>
  </li>
</ul>

---

<h3>New Files and Project Structure</h3>
<p>This phase introduced new folders and files to house the frontend code:</p>
<pre><code>
sih/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ ... (no changes here)
â”œâ”€â”€ static/                 &lt;-- NEW
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css       &lt;-- NEW
â”‚   â””â”€â”€ js/
â”‚       â””â”€â”€ main.js         &lt;-- NEW
â”œâ”€â”€ templates/              &lt;-- NEW
â”‚   â””â”€â”€ index.html          &lt;-- NEW
â”œâ”€â”€ app.py                  &lt;-- UPDATED
â””â”€â”€ ... (other root files)
</code></pre>

<h4>File Explanations</h4>
<ul>
  <li><code><b>templates/index.html</b></code>: This is the main HTML file that provides the structure for the entire web application, including the sidebar, chat window, and input form.</li>
  <li><code><b>static/css/style.css</b></code>: This file contains all the styling rules for the application, creating the simple dark theme, layout, and appearance of all components.</li>
  <li><code><b>static/js/main.js</b></code>: This is the "brain" of the frontend. It handles all user interactions, such as sending messages, communicating with the backend API, managing chat history in <code>localStorage</code>, and dynamically rendering messages and data tables in the chat window.</li>
  <li><code><b>app.py (Updated)</b></code>: The main application script was updated to serve the frontend. We added a new route (<code>@app.route('/')</code>) that tells Flask to render and display the <code>index.html</code> file when a user visits the root URL.</li>
</ul>

---

<h3>How to Run the Full Application (Backend + Frontend)</h3>
<ol>
  <li>Ensure your PostgreSQL database is running via Docker:
    <pre><code>docker-compose up -d</code></pre>
  </li>
  <li>Run the main Flask application from the root <code>sih/</code> directory:
    <pre><code>python app.py</code></pre>
  </li>
  <li>Open your web browser and navigate to the following URL:
    <pre><code>http://127.0.0.1:5000</code></pre>
  </li>
</ol>
<p>You should now see and be able to interact with the complete FloatChat application.</p>
