<!-- Main Title and Description -->
<h1 align="center">FloatChat - AI Conversational Interface for ARGO Data</h1>
<p align="center">
  This project is an AI-powered conversational system for ARGO float data that enables users to query, explore, and visualize oceanographic information using natural language.
</p>

---

<!-- Phase 1 Section -->
<h2>Phase 1: Data Foundation & Backend Setup</h2>
<p>
  This first phase focuses on building a robust data pipeline. The goal is to ingest raw ARGO NetCDF data files, process them into a structured format, and store them in a relational database (PostgreSQL) and a vector database (ChromaDB) for AI-powered retrieval.
</p>

<!-- Project Structure Section -->
<h3>Project Structure and File Explanations</h3>
<p>The file structure for this initial phase is as follows:</p>
<pre><code>
sih/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ data_processing/
â”‚   â”‚   â””â”€â”€ processor.py
â”‚   â””â”€â”€ database/
â”‚       â””â”€â”€ setup_db.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”œâ”€â”€ db/
â”œâ”€â”€ .env
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ run_ingestion.py
</code></pre>
<ul>
    <li><code><b>backend/</b></code>: This directory contains all the server-side Python logic, separated into modules.</li>
    <li><code><b>backend/data_processing/processor.py</b></code>: A script responsible for reading a single ARGO NetCDF (<code>.nc</code>) file, extracting key variables (like temperature, salinity), and converting them into a structured format (a pandas DataFrame).</li>
    <li><code><b>backend/database/setup_db.py</b></code>: This script connects to the PostgreSQL database and creates the necessary tables (<code>argo_floats</code> and <code>argo_measurements</code>) required to store the ARGO data.</li>
    <li><code><b>data/raw/</b></code>: This is where you must place the raw <code>.nc</code> ARGO data files you download. The ingestion script specifically looks in this folder.</li>
    <li><code><b>db/</b></code>: This directory is automatically generated by ChromaDB to store the local vector database files.</li>
    <li><code><b>.env</b></code>: A file to store sensitive information and environment variables, such as database credentials. This file is ignored by Git.</li>
    <li><code><b>docker-compose.yml</b></code>: Defines the services, networks, and volumes for our Docker application. In this phase, it's used to easily configure and run our PostgreSQL database.</li>
    <li><code><b>run_ingestion.py</b></code>: The main script for Phase 1. It orchestrates the entire pipeline: it finds data files in <code>data/raw/</code>, uses the <code>processor.py</code> to process them, and loads the structured data into both PostgreSQL and ChromaDB.</li>
</ul>

---

<!-- Installation Section -->
<h2>Step-by-Step Installation Guide</h2>

<h3>1. Prerequisites</h3>
<p>Ensure you have the following software installed on your system:</p>
<ul>
  <li><strong>Git:</strong> For cloning the repository.</li>
  <li><strong>Python 3.8+</strong> and Pip.</li>
  <li><strong>Docker Desktop:</strong> For running the PostgreSQL database in a container. It's the cleanest and most reliable way to manage the database.
    <ul>
      <li><a href="https://docs.docker.com/desktop/install/windows-install/" target="_blank">Install Docker on Windows</a></li>
      <li><a href="https://docs.docker.com/desktop/install/mac-install/" target="_blank">Install Docker on Mac</a></li>
      <li><a href="https://docs.docker.com/desktop/install/linux-install/" target="_blank">Install Docker on Linux</a></li>
    </ul>
  </li>
</ul>

<h3>2. Clone the Repository & Set Up Environment</h3>
<ol>
  <li><strong>Clone the repository:</strong>
    <pre><code>git clone &lt;your-repository-url&gt;
cd sih</code></pre>
  </li>
  <li><strong>Create and activate a virtual environment:</strong>
    <pre><code># On Windows
python -m venv venv
.\venv\Scripts\activate

# On macOS/Linux
python3 -m venv venv
source venv/bin/activate</code></pre>
  </li>
  <li><strong>Install the required Python packages:</strong>
    <pre><code>pip install -r requirements.txt</code></pre>
  </li>
</ol>

<h3>3. Configure Environment Variables</h3>
<ol>
  <li>In the root `sih/` folder, create a file named <strong>.env</strong>.</li>
  <li>Copy the following content into it. This password will be used to secure your database.</li>
</ol>
<pre><code>POSTGRES_USER=argo_user
POSTGRES_PASSWORD=your_secure_password
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=argo_db</code></pre>
<p><em><strong>Note:</strong> Replace <code>your_secure_password</code> with a password of your choice.</em></p>

<h3>4. Run the Database using Docker</h3>
<p>With Docker Desktop running, use Docker Compose to manage your database container. These commands should be run from the root <code>sih/</code> directory.</p>
<pre><code># Start the database container in the background
docker-compose up -d</code></pre>

<h4>Useful Docker Commands</h4>
<ul>
    <li><strong>Check container status:</strong>
        <pre><code>docker-compose ps</code></pre>
    </li>
    <li><strong>View real-time database logs (for debugging):</strong>
        <pre><code>docker-compose logs -f db</code></pre>
    </li>
    <li><strong>Stop and remove the container:</strong>
        <pre><code>docker-compose down</code></pre>
    </li>
</ul>


<h3>5. Download Data & Run the Ingestion Pipeline</h3>
<ol>
  <li><strong>Download sample ARGO data:</strong>
    <ul>
      <li>Go to the <a href="ftp://ftp.ifremer.fr/ifremer/argo/geo/indian_ocean/" target="_blank">Indian Ocean ARGO Data Repository</a>.</li>
      <li>Download a few <strong>.nc</strong> (NetCDF) files.</li>
      <li>Place these files inside the <strong><code>sih/data/raw/</code></strong> directory.</li>
    </ul>
  </li>
  <li><strong>Run the setup and ingestion scripts:</strong>
    <pre><code># First, create the tables in the database
python backend/database/setup_db.py

# Next, process the .nc files and load the data
python run_ingestion.py</code></pre>
  </li>
</ol>
<p>After the `run_ingestion.py` script finishes, Phase 1 is complete! Your database is now populated and ready for the AI backend.</p>

---

<!-- Prompt for Teammates Section -->
<h2>ðŸš€ Prompt for Teammates (Gemini/ChatGPT)</h2>
<p>To help other teammates get set up quickly, provide them with the following prompt to use in an AI assistant like Gemini or ChatGPT.</p>

<div style="background-color: #f0f0f0; border-left: 5px solid #007bff; padding: 15px; margin-top: 20px; font-family: monospace; white-space: pre-wrap;">
You are an expert technical assistant. Your goal is to guide me, a software developer, through setting up Phase 1 of the 'FloatChat' project on my local machine.

I have already cloned the project repository. Provide me with a clear, step-by-step set of commands and instructions to get the data pipeline running.

Use the following information to structure your response:
1.  **Prerequisites:** Remind me that I need Python 3.8+ and Docker Desktop installed. Provide the official download links for Docker.
2.  **Virtual Environment:** Give me the commands to create and activate a Python virtual environment for both Windows and macOS/Linux.
3.  **Dependencies:** Tell me the command to install the required packages using the `requirements.txt` file.
4.  **Environment Configuration:** Instruct me to create a `.env` file in the project root and provide the exact content it should have. Tell me to set a secure password.
5.  **Database Setup:** Provide the `docker-compose up -d` command to start the PostgreSQL server and explain briefly what it does.
6.  **Data Ingestion:** Instruct me to download sample ARGO `.nc` files from `ftp://ftp.ifremer.fr/ifremer/argo/geo/indian_ocean/` and place them in the `sih/data/raw/` folder.
7.  **Run the Pipeline:** Provide the final two Python commands (`python backend/database/setup_db.py` and `python run_ingestion.py`) in the correct order and explain what each one does.

Please be concise and use markdown code blocks for all terminal commands.
</div>
